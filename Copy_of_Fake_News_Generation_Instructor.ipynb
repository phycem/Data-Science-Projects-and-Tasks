{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phycem/Data-Science-Projects-and-Tasks/blob/main/Copy_of_Fake_News_Generation_Instructor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7XZXnjfZ9p"
      },
      "source": [
        "# Fake News Generation\n",
        "\n",
        "In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news.\n",
        "\n",
        "**Check out some examples of AI-written fake news [here](https://notrealnews.net/)!**\n",
        "\n",
        "**Discuss:** How is fake news created and distributed? What are the risks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9IoN6bibZj"
      },
      "source": [
        "**Before starting, set your runtype type to GPU!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUgfksN_iZR"
      },
      "source": [
        "##Outline\n",
        "\n",
        "We'll build RNNs to predict language character-by-character to generate fake news! We'll:\n",
        "\n",
        "\n",
        "* Encode our text data for the language model\n",
        "* Build, train, and explore RNN and LSTM models\n",
        "* Advanced: Create visualizations of our model's confidence\n",
        "* Optional: compare our results to a state of the art word-wise language model, GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-THemqM_Uy_C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "from collections import Counter\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!wget \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/fake.txt\"\n",
        "!wget \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/pre_train.zip\"\n",
        "\n",
        "! unzip -oq pre_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGFprDdkVJFd"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to load some helper functions\n",
        "def load_data():\n",
        "    with open(\"fake.txt\", \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def simplify_text(text, vocab):\n",
        "    new_text = \"\"\n",
        "    for ch in text:\n",
        "        if ch in vocab:\n",
        "            new_text += ch\n",
        "    return new_text\n",
        "\n",
        "def sample_from_model(\n",
        "    model,\n",
        "    text,\n",
        "    char_indices,\n",
        "    chunk_length,\n",
        "    number_of_characters,\n",
        "    seed=\"\",\n",
        "    generation_length=400,\n",
        "):\n",
        "    indices_char = {v: k for k, v in char_indices.items()}\n",
        "    for diversity in [0.2, 0.5, 0.7]:\n",
        "        print(\"----- diversity:\", diversity)\n",
        "        generated = \"\"\n",
        "        if not seed:\n",
        "            text = text.lower()\n",
        "            start_index = random.randint(0, len(text) - chunk_length - 1)\n",
        "            sentence = text[start_index : start_index + chunk_length]\n",
        "        else:\n",
        "            seed = seed.lower()\n",
        "            sentence = seed[:chunk_length]\n",
        "            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for _ in range(generation_length):\n",
        "            x_pred = np.zeros((1, chunk_length, number_of_characters))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "class SampleAtEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n",
        "        self.data = data\n",
        "        self.char_indices = char_indices\n",
        "        self.chunk_length = chunk_length\n",
        "        self.number_of_characters = number_of_characters\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        sample_from_model(\n",
        "            self.model,\n",
        "            self.data,\n",
        "            self.char_indices,\n",
        "            self.chunk_length,\n",
        "            self.number_of_characters,\n",
        "            generation_length=200,\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_str(model, text, char2indices, top=10, graph_mode = True):\n",
        "    if text == '':\n",
        "      print(\"waiting...\")\n",
        "      return\n",
        "    text = text.lower()\n",
        "    assert len(text) <= CHUNK_LENGTH\n",
        "    oh = np.array([one_hot_sentence(text, char2indices)])\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      pred = model.predict(oh).flatten()\n",
        "    sort_indices = np.argsort(pred)[::-1][:top]\n",
        "    if graph_mode:\n",
        "      plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n",
        "      plt.title(f\"Predicted probabilities of the character following '{text}'\")\n",
        "      plt.show()\n",
        "    else:\n",
        "      return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvr9mfPLgHZf"
      },
      "source": [
        "## Language models\n",
        "\n",
        "A language model tries to learn how language works. Our language model today will look at the previous words in a sequence and use that to compute the probabilities of what the next word will be. Actually, our model will do something even more fundamental: it'll try to predict what the next character in sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6RTa9-2U-sC"
      },
      "outputs": [],
      "source": [
        "#@title Run to load the vocabulary\n",
        "\n",
        "# VOCABULARY defines the set of acceptable characters that the model can handle\n",
        "# CORPUS_LENGTH is how long our training dataset is\n",
        "# CHUNK_LENGTH is how many characters previously our model can remember\n",
        "# CHAR2INDICES is a mapping from characters to their indices in the one hot encoding\n",
        "\n",
        "STEP = 3\n",
        "LEARNING_RATE = 0.0005\n",
        "CORPUS_LENGTH = 200000\n",
        "CHUNK_LENGTH = 40\n",
        "VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n",
        "print(VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5N4kVBHivkR"
      },
      "source": [
        "Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xNZ-FRjVJDk"
      },
      "outputs": [],
      "source": [
        "data = load_data()\n",
        "data = data[:CORPUS_LENGTH]\n",
        "data = simplify_text(data, CHAR2INDICES)\n",
        "print(f\"Type of the data is: {type(data)}\\n\")\n",
        "print(f\"Length of the data is: {len(data)}\\n\")\n",
        "print(f\"The first couple of sentences of the data are:\\n\")\n",
        "print(data[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8xSZrQZ_ugW"
      },
      "source": [
        "## Discussion 1\n",
        "\n",
        "What does `len(data)` tell us?\n",
        "\n",
        "a. number of sentences in our data\n",
        "\n",
        "b. number of words in our data\n",
        "\n",
        "c. number of characters in our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5u33MrnjcXj"
      },
      "source": [
        "## Encoding words\n",
        "\n",
        "Before we can do any machine learning, we'll have to encode our data in numbers. Just like in the Yelp review notebook, we'll be using one hot encodings - but with two differences:\n",
        "\n",
        "1. This time, the vocabulary is the set of characters instead of words. \n",
        "\n",
        "2. In text generation, we care a lot about context/order - so we won't use the Bag of Words model, where we just add up the one hot vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezO-hpg8rb4K"
      },
      "source": [
        "### Exercise 1\n",
        "We want to make a one-hot vector for a given character.  For example, the one-hot encoding for 'b' is:\n",
        "\n",
        "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJuumbw4S7wA"
      },
      "outputs": [],
      "source": [
        "print(CHAR2INDICES)\n",
        "#How does this help us?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwN7fo1IWlgL"
      },
      "outputs": [],
      "source": [
        "def one_hot(char, char_indices): #char_indices arg will be fill by CHAR2INDICES, shown above\n",
        "    num_chars = len(char_indices)\n",
        "    vec = [0] * num_chars # Start off with a vector of all 0s\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    # Your task: where in vec does the 1 go?\n",
        "    \n",
        "    ### END YOUR CODE ###\n",
        "    return vec\n",
        "\n",
        "\n",
        "def one_hot_sentence(sentence, char_indices):\n",
        "    return [one_hot(c, char_indices) for c in sentence]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O6BYv45fKc0I"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "\n",
        "# Complete the implementation of the one_hot function, which takes in a single character as input, and then returns a one hot vector for that character, which is a list with zeros everywhere, except a 1 in the index for that character. The mapping between charcters and indices are stored in the argument char_indices\n",
        "\n",
        "def one_hot(char, char_indices):\n",
        "    num_chars = len(char_indices)\n",
        "    vec = [0] * num_chars # Start off with a vector of all 0s\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    # Your task: where in vec does the 1 go?\n",
        "    vec[char_indices[char]] = 1\n",
        "    ### END YOUR CODE ###\n",
        "    return vec\n",
        "\n",
        "\n",
        "def one_hot_sentence(sentence, char_indices):\n",
        "    return [one_hot(c, char_indices) for c in sentence]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjNBrFRklFuA"
      },
      "source": [
        "When you've got it, test it below, try typing 'abc', and see if you get what you would expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BouniNa5lE44"
      },
      "outputs": [],
      "source": [
        "interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"abc\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngmxrKtC9I03"
      },
      "source": [
        "### Exercise 2\n",
        "Let's make sure we understand what one_hot_sentence is doing by printing its shape and figuring out what the dimensions mean - a common practice in coding and debugging!\n",
        "\n",
        "Print the dimensions of abc_encoded.  What do they mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EmK7jbx9brZ"
      },
      "outputs": [],
      "source": [
        "abc_encoded = np.array(one_hot_sentence('abc', CHAR2INDICES))\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnp1rh0QK4PA"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "abc_encoded = np.array(one_hot_sentence('abc', CHAR2INDICES))\n",
        "abc_encoded.shape\n",
        "# first dim: #chars to encode; second dim: #chars in vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEwPDLvsmdEk"
      },
      "source": [
        "## Building the Language Model\n",
        "\n",
        "We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "seiOTRwNWrPZ"
      },
      "outputs": [],
      "source": [
        "#@title Run to extract x and y, the input and output to the model, from the raw text.\n",
        "def get_x_y(text, char_indices):\n",
        "    \"\"\"\n",
        "    Extracts x and y from the raw text.\n",
        "    \n",
        "    Arguments:\n",
        "        text (str): raw text\n",
        "        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n",
        "\n",
        "    Returns:\n",
        "        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n",
        "    \n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n",
        "        sentences.append(text[i : i + CHUNK_LENGTH])\n",
        "        next_chars.append(text[i + CHUNK_LENGTH])\n",
        "\n",
        "    print(\"Chunk length:\", CHUNK_LENGTH)\n",
        "    print (\"Step size:\", STEP)\n",
        "    print(\"Number of chunks:\", len(sentences))\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        x.append(one_hot_sentence(sentence, char_indices))\n",
        "        y.append(one_hot(next_chars[i], char_indices))\n",
        "\n",
        "    return np.array(x, dtype=bool), np.array(y, dtype=bool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmonfcQnlW0U"
      },
      "source": [
        "Let's check out `x` and `y`! Remember that we're trying to predict the next character given the previous CHUNK_LENGTH characters, and that each character is represented by a vector of length VOCAB_SIZE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZG_6eOCVjDV"
      },
      "outputs": [],
      "source": [
        "print(\"This might take a while...\")\n",
        "x, y = get_x_y(data, CHAR2INDICES)\n",
        "print(\"Shape of x is\", x.shape)\n",
        "print(\"Shape of y is \", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7C5B3_ihBS8"
      },
      "source": [
        "### Discussion 2\n",
        "\n",
        "Can you explain the shapes of `x` and `y`? How does each entry of `x` relate to the corresponding entry in `y`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehB76k6rgav"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Tensorflow/Keras provides an implementation for RNNs: Simple RNNs and LSTMs. \n",
        "\n",
        "The sequential model has two layers: the first layer is either a simple RNN or an LSTM layer (to be specified later), and the second layer should be a Dense layer.  Remember to use `model.add()` to add a layer!\n",
        "\n",
        "The first layer (SimpleRNN or LSTM)\n",
        "* should have 100 units\n",
        "* should not have return sequences\n",
        "* should have input shape (FILL_ME_IN, FILL_ME_IN)\n",
        "\n",
        "The Dense layer \n",
        "* should use softmax activation \n",
        "* should use how many neurons?\n",
        "\n",
        "You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvyc7aqdVjF_"
      },
      "outputs": [],
      "source": [
        "def get_model(chunk_length, number_of_characters, lr, architecture): \n",
        "    model = tf.keras.Sequential()\n",
        "    if architecture=='rnn':\n",
        "      ### YOUR CODE HERE\n",
        "      pass\n",
        "      ### END CODE\n",
        "    elif architecture=='lstm':\n",
        "      ### YOUR CODE HERE\n",
        "      pass \n",
        "      ### END CODE\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(number_of_characters, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLSDHdz2LQMo"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "#input_shape = (chunk_length, number_of_characters)\n",
        "#neurons in dense layer = number_of_characters\n",
        "\n",
        "def get_model(chunk_length, number_of_characters, lr, architecture): \n",
        "    model = tf.keras.Sequential()\n",
        "    if architecture=='rnn':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(\n",
        "          tf.keras.layers.SimpleRNN(\n",
        "              100,\n",
        "              return_sequences=False,\n",
        "              input_shape=(chunk_length, number_of_characters),\n",
        "          )\n",
        "      )\n",
        "    ### END CODE\n",
        "    elif architecture=='lstm':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(\n",
        "          tf.keras.layers.LSTM(\n",
        "              100,\n",
        "              return_sequences=False,\n",
        "              input_shape=(chunk_length, number_of_characters),\n",
        "          )\n",
        "      )\n",
        "    ### END CODE\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(number_of_characters, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWUb0OC32bLq"
      },
      "source": [
        "Let's check out our model's structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "firMyjYIVjLB"
      },
      "outputs": [],
      "source": [
        "ARCHITECTURE = 'rnn'\n",
        "model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2XOpqLnpHq"
      },
      "source": [
        "# Fitting the model \n",
        "Great! Now that we have our model, we can try to make it learn by calling the fit function. The callback here just samples the model before every pass through the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3DUysfrmng"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "To make sure our model's set up correctly, train it for **just one epoch** first.\n",
        "\n",
        "What interesting things do you see? What is the model's behavior before training? How about after 1 epoch?\n",
        "\n",
        "\n",
        "You will need 4 parameters to model.fit():\n",
        "\n",
        "1. input variable\n",
        "\n",
        "2. output variable\n",
        "\n",
        "3. callbacks=[sample_callback]\n",
        "\n",
        "4. epochs=? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmB25PfBVjBN"
      },
      "outputs": [],
      "source": [
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "###YOUR CODE HERE### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1Pk21doLmlv"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "###Your code here###\n",
        "\n",
        "#solution\n",
        "model.fit(\n",
        "    x, y, callbacks=[sample_callback], epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avOy6lEenOdN"
      },
      "source": [
        "### Discussion 3\n",
        "\n",
        "How's your model doing so far? What does the model learn as it trains?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFENyrkLCJ3k"
      },
      "source": [
        "### Discussion 4\n",
        "\n",
        "As you might notice, training a model from scratch is slow!\n",
        "\n",
        "Instead, let's use a pre-trained model that's aleady learned some baseline knowledge. On top of this we can finetune the model using our own data.  Why is this helpful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDSTnnCQXZfH"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"cp.ckpt/\")\n",
        "\n",
        "#YOUR CODE HERE to continue training - similar to when you trained for 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4BQD3h6nYOA"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "model = tf.keras.models.load_model(\"cp.ckpt/\")\n",
        "\n",
        "#YOUR CODE HERE to continue training the pre-trained model\n",
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "\n",
        "model.fit(\n",
        "    x, y, callbacks=[sample_callback], epochs=3,\n",
        ") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJ8zO9khq8v"
      },
      "source": [
        "### Discussion 5\n",
        "\n",
        "*   What has and hasn't our model learned?\n",
        "*   What does `diversity` seem to represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqLvCiA7pLuh"
      },
      "source": [
        "### What has our model learned? \n",
        "\n",
        "From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (thought makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lenghts at least. What other things about grammar does it know?\n",
        "\n",
        "Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n",
        "\n",
        "\n",
        "*   Has it learned that the letter that follows 'q' is usually a 'u'?\n",
        "*   What is the most likely letter after 'fb'?\n",
        "*   What is the most likely letter after 'th'?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_tZ2k93cdyK"
      },
      "outputs": [],
      "source": [
        "interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwr_BlXUn4sw"
      },
      "source": [
        "# Further Exploration\n",
        "\n",
        "Take a moment now to do some exploration via Exercise 5 and 6, or anything else you'd like to try!\n",
        "\n",
        "In addition, you might want to go back to Exercise 3 and use model.add() to stack the RNN/LSTM layers on top of one another!  Check out: \n",
        "\n",
        "\n",
        "*   Last example at [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n",
        "*   [StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzQb6to3Z4h"
      },
      "source": [
        "## Exercise 5\n",
        "\n",
        "Try omitting special characters (e.g., punctuation, digits) from the vocabulary! Does it make things easier? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "989Y50Vl3cnZ"
      },
      "outputs": [],
      "source": [
        "#before: VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "SMALL_VOCAB = None ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEoRJUYHEjtn"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "SMALL_VOCAB = string.ascii_lowercase + \" \\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9vyUhMdcso"
      },
      "source": [
        "Now, we can train our model with this code copied from earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA-aEKgA7PuG"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "VOCAB_SIZE = len(SMALL_VOCAB)\n",
        "SMALL_CHAR2INDICES = dict(zip(SMALL_VOCAB, range(len(SMALL_VOCAB))))\n",
        "print(SMALL_VOCAB)\n",
        "\n",
        "data_nv = load_data()\n",
        "data_nv = simplify_text(data_nv[:CORPUS_LENGTH], SMALL_CHAR2INDICES)\n",
        "x_nv, y_nv = get_x_y(data_nv, SMALL_CHAR2INDICES)\n",
        "\n",
        "model_nv = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, 'rnn')\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_nv.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSHnsM5O8FMn"
      },
      "outputs": [],
      "source": [
        "interact(lambda sequence: predict_str(model_nv, sequence, SMALL_CHAR2INDICES), sequence='th');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFRn294L2QG"
      },
      "source": [
        "Discuss:\n",
        "\n",
        "What changed? Hint: look at the numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8m5XeWhzm-a"
      },
      "source": [
        "## Exercise 6\n",
        "\n",
        "Using the simplified vocabulary, let's compare how the first 3 epochs of learning go for the SimpleRNN vs. the LSTM.  Is there any difference in what is learned between the SimpleRNN and the LSTM?\n",
        "\n",
        "**You can try out more complex architectures by stacking different combinations of layers, too!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAHtvv7WztfQ"
      },
      "outputs": [],
      "source": [
        "#YOUR CODE HERE to try out RNN vs. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huav6KSeImOr"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "ARCHITECTURE = 'rnn'\n",
        "model_rnn = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_rnn.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) \n",
        "\n",
        "ARCHITECTURE = 'lstm'\n",
        "model_lstm = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_lstm.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpcMDJY5y51q"
      },
      "source": [
        "##Discussion: Text Generation\n",
        "\n",
        "Based on what you've seen today...\n",
        "\n",
        "\n",
        "*   What could be some techniques for generating fake news? What are the dangers?\n",
        "*   How could we detect or prevent fake news?\n",
        "*   What are some uses of text generation for good?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlpNFkIsVGI"
      },
      "source": [
        "#Challenge: Visualizing Model Confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrtLJIetv7k"
      },
      "source": [
        "As always, it can be hard to understand how the model makes its decisions! Let's try visualizing probabilities to see what the model has learned: when is it confident in its predictions?\n",
        "\n",
        "We'll make a visualization like the **red** squares [here, under \"Visualizing the predictions and the “neuron” firings in the RNN\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/#Visualizing). **Check out that graphic and discuss: what does each square represent? How could we make this?**\n",
        "\n",
        "Let's jump in! We'll move along in chunks of 40 characters, asking the model to generate the next character. First, some useful constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6uBZEskq1g-"
      },
      "outputs": [],
      "source": [
        "to_gen = 30 #Generate 30 new characters\n",
        "start = 0 #Start at the beginning of data \n",
        "vocab_list = list(VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DaTYsCSutnY"
      },
      "source": [
        "Now, let's get our predictions! First, let's learn to interpret the output. Here's a useful line of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4XEZ-w_vK2q"
      },
      "outputs": [],
      "source": [
        "preds = predict_str(model, \"this is a test chunk of forty characters\", CHAR2INDICES, graph_mode=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOYLL4R0vi5F"
      },
      "source": [
        "Using `preds` and `vocab_list`, what are the model's top 5 choices for the next character? What's the probability for each one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pjXPFFDwHX8"
      },
      "outputs": [],
      "source": [
        "\n",
        "pred_series = pd.Series(preds, index = vocab_list).sort_values(ascending = False).iloc[:5]\n",
        "print (pred_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpCLlbqBwSwg"
      },
      "source": [
        "Now, let's use that to make predictions for sliding 40-character chunks! Please start at `start` and move along `data` one character at a time. For each 40-character chunk, you should store:\n",
        "\n",
        "*   the last character of the chunk in `last_char`\n",
        "*   the model's five most likely new characters in `pred_char`\n",
        "*   the probabilities for those five characters in  `pred_prob`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssPjJ40_unxc"
      },
      "outputs": [],
      "source": [
        "last_char = [] #Final size: to_gen\n",
        "pred_char = [] #Final size: to_gen x 5 \n",
        "pred_prob = [] #Final size: to_gen x 5\n",
        "\n",
        "#YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t46w0oCixQQk"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "for i in range(to_gen):\n",
        "  data_chunk = data[start + i:start+i+CHUNK_LENGTH]\n",
        "  preds = predict_str(model, data_chunk, CHAR2INDICES, graph_mode=False) \n",
        "  pred_series = pd.Series(preds, index = vocab_list).sort_values(ascending = False).iloc[:5]\n",
        "  last_char.append(data_chunk[-1])\n",
        "  pred_char.append(list(pred_series.index))\n",
        "  pred_prob.append(list(pred_series))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8MIMH27xd0X"
      },
      "source": [
        "Finally, we can make our visualization. The code below will plot the probabilities and show you how to add text. Please fill in all the text using `last_char` and `pred_char`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7taVWC3fmstB"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize = [20,100])\n",
        "pred_array = np.array(pred_prob)\n",
        "pred_array = np.insert(pred_array,0,0,1) #Add extra row\n",
        "ax.imshow(pred_array.T, cmap = 'Reds')\n",
        "\n",
        "#YOUR CODE HERE to fill in text\n",
        "plt.text(6,3,\"A\",fontsize='xx-large') #This is how you add text\n",
        "#END YOUR CODE\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z7_H4Kg9ybUn"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "fig, ax = plt.subplots(figsize = [20,100])\n",
        "pred_array = np.array(pred_prob)\n",
        "pred_array = np.insert(pred_array,0,0,1)\n",
        "ax.imshow(pred_array.T, cmap = 'Reds')\n",
        "\n",
        "#YOUR CODE HERE to fill in text\n",
        "for ir, row in enumerate(pred_char):\n",
        "  for ic, char in enumerate(row):\n",
        "    plt.text(ir, ic+1, char,fontsize='xx-large')\n",
        "for i, char in enumerate(last_char):\n",
        "  plt.text(i,0,char,fontsize='xx-large')\n",
        "\n",
        "\n",
        "#END YOUR CODE\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykVCO0dyGOu"
      },
      "source": [
        "If you've completed this visualization, nice work! **What do you notice?** When is your model confident in its predictions, and when not? When does it mess up badly?\n",
        "\n",
        "As a bonus, try changing your code so that you can provide your own input text, and see what patterns you notice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUR6G-cM_Qk"
      },
      "source": [
        "#Optional: GPT-2 Transformer Model\n",
        "\n",
        "Please run the following cell (Getting GPT-2 Up and Running) as you discuss GPT-2, since it takes a while to execute.  \n",
        "\n",
        "**Technical Note:** This section uses a different versions of some libraries than the rest of the notebook. If you're having issues, please either:\n",
        "*   Copy this code over into a new notebook, OR\n",
        "*   \"Factory reset runtime\" and then run this section. If you need to go back to the beginning of the notebook, reset again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm1SoJXxMG9M"
      },
      "outputs": [],
      "source": [
        "#@title Run: Getting GPT-2 Up and Running\n",
        "\"\"\"\n",
        "Install the GPT-2 fine-tuning library\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install -q gpt-2-simple\n",
        "#!pip3 install gast==0.2.2\n",
        "!pip3 install -q tensorflow==1.15\n",
        "\n",
        "\"\"\"\n",
        "Import libraries\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from zipfile import ZipFile\n",
        "import gpt_2_simple as gpt2\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4.element import Comment\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "\"\"\"\n",
        "Get the training data link\n",
        "\"\"\"\n",
        "\n",
        "site = 'https://www.dropbox.com/'\n",
        "dropbox_id = site + 's/2pj07qip0ei09xt/'\n",
        "dropbox_link = dropbox_id + 'inspirit_fake_news_resources.zip?dl=1'\n",
        "\n",
        "\"\"\"\n",
        "Extract the data from the DropBox link\n",
        "\"\"\"\n",
        "\n",
        "r = requests.get(dropbox_link)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "\"\"\"\n",
        "Get the pickled data from the ZIP file\n",
        "\"\"\"\n",
        "\n",
        "z.extractall()\n",
        "basepath = '.'\n",
        "path = os.path.join(basepath, 'train_val_data.pkl')\n",
        "\n",
        "\"\"\"\n",
        "Load the pickle files with training and validation data\n",
        "\"\"\"\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "Define functions to extract visible text from website HTML\n",
        "\"\"\"\n",
        "\n",
        "def text_from_html(body):\n",
        "    soup = bs(body, 'html.parser')\n",
        "    texts = soup.findAll(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)  \n",
        "    return ' '.join((u\" \".join(t.strip() for t in visible_texts)).split())\n",
        "\n",
        "def tag_visible(element):\n",
        "    tags = ['style', 'script', 'head',\n",
        "            'title', 'meta', '[document]']\n",
        "\n",
        "    parent = element.parent.name\n",
        "    if parent in tags: return False\n",
        "    if isinstance(element, Comment): return False\n",
        "    if parent not in tags and not isinstance(element, Comment): return True \n",
        "\n",
        "\"\"\"\n",
        "Create a string with all real news from the dataset\n",
        "\"\"\"\n",
        "\n",
        "news = ''\n",
        "\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(train_data) if data_point[2]==0)\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(val_data) if data_point[2]==0)\n",
        "\n",
        "# for data_point in tqdm(train_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' '\n",
        "\n",
        "# for data_point in tqdm(val_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' ' \n",
        "\n",
        "\"\"\"\n",
        "Load the GPT-2 model with pre-trained weights\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"124M\"\n",
        "print(f\"Downloading {model_name} model...\")\n",
        "gpt2.download_gpt2(model_name = model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IucOs2lPjBcR"
      },
      "source": [
        "<center><img src=\"https://imgur.com/p16AuJH.jpg\" width=\"1000px\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jri_GRGtpAM-"
      },
      "source": [
        "In the remaining code blocks, we build a fake news generation model based on GPT-2 (a transformer model). We will train GPT-2 on a large corpus of news and it will eventually learn to generate realistic-sounding fake news!  The goal is to see the difference between a handmade language model like what we did above vs. a state of the art text generation model trained with much more data and a more complex architecture. Most of the code is given because it is very specific to GPT-2, but please read through it and ask questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLZHpcIQcJv"
      },
      "source": [
        "Above, we used two types of RNNs: Simple RNNs and LSTMs.  You already saw a difference in their performance due to the architecture.  Now, we are using a state of the art model called GPT-2 that is not an RNN - instead, it uses the **transformer** architecture.  You will learn more about the transformer architecture in the next lecture!\n",
        "\n",
        "You can check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzu4t5yArMtj"
      },
      "source": [
        "### Fine-tune the GPT-2 model\n",
        "\n",
        "Next, we dump all the news into a *.txt* file and fine-tune *GPT-2* on this text. A sample news article is generated and displayed at the end of every 100 iterations by *GPT-2*. Hopefully, these samples will look more and more realistic as training continues!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wZS0cXUxb7vG"
      },
      "outputs": [],
      "source": [
        "#Dump the text into a .txt file and fine-tune the model\n",
        "\n",
        "news = news[:-1]\n",
        "file_name = 'news.txt'\n",
        "with open(file_name, 'w') as f: f.write(news)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, file_name,\n",
        "              model_name=model_name, steps=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ag1U3Bvraz-"
      },
      "source": [
        "### Exercise 7: Test the model\n",
        "\n",
        "Now, we test the model by generating 10 sample fake news article. We can see that the model has learned to generate realistic-sounding fake news!\n",
        "\n",
        "use `gpt2.generate(sess)` to generate an example, and use a for loop to do more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "houImEKYL4gO"
      },
      "outputs": [],
      "source": [
        "### Your code here ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yAGegRsxPetD"
      },
      "outputs": [],
      "source": [
        "#@title Solution Hidden\n",
        "for i in range(1, 11):\n",
        "    print(str(i)+\")\\n\")\n",
        "    gpt2.generate(sess)\n",
        "    print(\"\\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnAplM3NU2I"
      },
      "source": [
        "##Discussion 6\n",
        "\n",
        "How did GPT2 do in comparison with our handmade language model?  Why do you think so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCWRiYpOSyD"
      },
      "source": [
        "##Discussion 7\n",
        "Check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/). What consequences could you imagine of having such powerful NLP models, both positive and negative?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}